{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1fgfLUjREL4GUJWEECTBXj4rhJSxluqRu",
      "authorship_tag": "ABX9TyO64JJrYfE7my01mJBfpSoW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HugoLeda/ML-Olympiad-Toxic-Language-PTBR-Detection/blob/main/Application.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "import pickle\n",
        "import joblib\n",
        "import requests\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer"
      ],
      "metadata": {
        "id": "Anch5EI1Uo2E"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1hK_My1WBZY",
        "outputId": "fcc26deb-8b64-463d-9681-f2e9e846d529"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wget"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TpM1QmfQEP6",
        "outputId": "3f78b806-8787-4ea1-aa87-69b3206a3722"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wget in /usr/local/lib/python3.10/dist-packages (3.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_repeated_chars(text):\n",
        " return re.sub(r\"(.)\\1{2,}\", r\"\\1\", text)"
      ],
      "metadata": {
        "id": "oGMwpCPfUo4x"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cleanText(text):\n",
        "  text = text.encode('ascii', 'ignore').decode('ascii') #remove emojis\n",
        "  text = re.sub(r'@\\w+', '', text) # remove users mentions\n",
        "  text = re.sub(r'htttps?//\\S+', '', text) #remove links\n",
        "  text = re.sub(r'\\s+', ' ', text) #remove extra spaces\n",
        "  text = re.sub(r'\\b(rt|user|https)\\b', '', text, flags=re.IGNORECASE) #remove some words\n",
        "  text = remove_repeated_chars(text)\n",
        "\n",
        "  return text.strip()"
      ],
      "metadata": {
        "id": "S3P1F0DdUo70"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def removeStopWords(text):\n",
        "  stopWords = set(stopwords.words('portuguese'))\n",
        "  text = text.lower()\n",
        "  text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "  words = text.split()\n",
        "\n",
        "  filteredWords = [word for word in words if word.lower() not in stopWords]\n",
        "\n",
        "  cleanedText = ' '.join(filteredWords)\n",
        "\n",
        "  return cleanedText"
      ],
      "metadata": {
        "id": "J7MUqW_GUo-t"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def portugueseStemmer(text):\n",
        "  stemmer = SnowballStemmer('portuguese')\n",
        "\n",
        "  words = text.split()\n",
        "  stemmedWords = [stemmer.stem(word) for word in text.split()]\n",
        "  stemmedText = ' '.join(stemmedWords)\n",
        "\n",
        "  return stemmedText"
      ],
      "metadata": {
        "id": "xtmYaR6fUpBk"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/HugoLeda/ML-Olympiad-Toxic-Language-PTBR-Detection/main/assets/toxic_words.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHMYk9hpPxEH",
        "outputId": "f18532ee-fc38-49f0-a8d8-efdd3967c635"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-06-09 23:57:10--  https://raw.githubusercontent.com/HugoLeda/ML-Olympiad-Toxic-Language-PTBR-Detection/main/assets/toxic_words.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 70725 (69K) [text/plain]\n",
            "Saving to: ‚Äòtoxic_words.csv.1‚Äô\n",
            "\n",
            "toxic_words.csv.1   100%[===================>]  69.07K  --.-KB/s    in 0.08s   \n",
            "\n",
            "2024-06-09 23:57:11 (884 KB/s) - ‚Äòtoxic_words.csv.1‚Äô saved [70725/70725]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/HugoLeda/ML-Olympiad-Toxic-Language-PTBR-Detection/main/assets/non_toxic_words.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdTCXLd4QYMM",
        "outputId": "a7c83aff-6a76-4ef7-c809-00d6d21ae5e8"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-06-09 23:57:11--  https://raw.githubusercontent.com/HugoLeda/ML-Olympiad-Toxic-Language-PTBR-Detection/main/assets/non_toxic_words.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 70725 (69K) [text/plain]\n",
            "Saving to: ‚Äònon_toxic_words.csv.1‚Äô\n",
            "\n",
            "non_toxic_words.csv 100%[===================>]  69.07K  --.-KB/s    in 0.08s   \n",
            "\n",
            "2024-06-09 23:57:11 (898 KB/s) - ‚Äònon_toxic_words.csv.1‚Äô saved [70725/70725]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('toxic_words.csv')\n",
        "toxic_words = df['word'].tolist()\n",
        "\n",
        "df = pd.read_csv('non_toxic_words.csv')\n",
        "non_toxic_words = df['word'].tolist()"
      ],
      "metadata": {
        "id": "iTs2zSUcQic7"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_neutral_words(text):\n",
        "  count = 0\n",
        "  for word in text.split():\n",
        "    if ((word in non_toxic_words) and (word in toxic_words)):\n",
        "      count += 1\n",
        "  return count"
      ],
      "metadata": {
        "id": "1i74BVLswm9D"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_toxic_words(text):\n",
        "  count = 0\n",
        "  for word in text.split():\n",
        "    if word in toxic_words:\n",
        "      count += 1\n",
        "  return count"
      ],
      "metadata": {
        "id": "zjOx2kQuwuoC"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_non_toxic_words(text):\n",
        "  count = 0\n",
        "  for word in text.split():\n",
        "    if word in non_toxic_words:\n",
        "      count += 1\n",
        "  return count"
      ],
      "metadata": {
        "id": "88NLfzSywyXr"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = joblib.load('/content/drive/MyDrive/Fatec/Aprendizado de Maquina/O plano/lr_model.pkl')\n",
        "tfidf_vectorizer = joblib.load('/content/drive/MyDrive/Fatec/Aprendizado de Maquina/O plano/tfidf_vectorizer.pkl')"
      ],
      "metadata": {
        "id": "iculb-iJvS5D"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def makePredict(df):\n",
        "  df['text'] = df['text'].apply(cleanText)\n",
        "  df['text'] = df['text'].apply(removeStopWords)\n",
        "  df['text'] = df['text'].apply(portugueseStemmer)\n",
        "\n",
        "  df['count_toxic_words'] = df['text'].apply(count_toxic_words)\n",
        "  df['count_non_toxic_words'] = df['text'].apply(count_non_toxic_words)\n",
        "  df['count_neutral_words'] = df['text'].apply(count_neutral_words)\n",
        "\n",
        "  df['count_char'] = df['text'].apply(lambda x: len(x))\n",
        "  df['count_words'] = df['text'].apply(lambda x: len(x.split()))\n",
        "\n",
        "  X_text = df['text']\n",
        "  X_other_features = df[['count_toxic_words', 'count_non_toxic_words', 'count_neutral_words', 'count_char', 'count_words']]\n",
        "\n",
        "  X_test_tfidf = tfidf_vectorizer.transform(X_text)\n",
        "\n",
        "  from scipy.sparse import hstack\n",
        "  X = hstack([X_test_tfidf, X_other_features])\n",
        "  y = df['label']\n",
        "\n",
        "  y_pred_test = model.predict(X)\n",
        "\n",
        "  df['label'] = y_pred_test\n",
        "\n",
        "  df.head()"
      ],
      "metadata": {
        "id": "tYxqY4dqU0VJ"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Verificar se tweets podem ser t√≥xicos\\n-------------------------------------')\n",
        "print('\\n\\nPara parar a execu√ß√£o digite \"s\"')\n",
        "\n",
        "run = True\n",
        "tweets = []\n",
        "\n",
        "while (run == True):\n",
        "  text = input('Digite um tweet para adicionar a lista de verifica√ß√£o: ')\n",
        "  if (text.upper() == 'S'):\n",
        "    run = False\n",
        "  else:\n",
        "    tweets.append(text)\n",
        "\n",
        "df = pd.DataFrame({\n",
        "  'id': range(len(tweets)),\n",
        "  'text': tweets,\n",
        "  'label': ''\n",
        "})\n",
        "\n",
        "makePredict(df)\n",
        "\n",
        "print('\\n')\n",
        "for i, tweet in enumerate(tweets):\n",
        "  label_description = (lambda x: 't√≥xico' if x == 1 else 'n√£o t√≥xico')(df['label'][i])\n",
        "  print(f'O tweet: \"{tweet}\", √© classificado como: \"{label_description}\"')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nb-pmgQhZMvt",
        "outputId": "97312150-48b9-41ab-c5e8-852822f82182"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verificar se tweets podem ser t√≥xicos\n",
            "-------------------------------------\n",
            "\n",
            "\n",
            "Para parar a execu√ß√£o digite \"s\"\n",
            "Digite um tweet para adicionar a lista de verifica√ß√£o: A China acabou de dar um grande passo √† frente em IA.  Eles lan√ßaram um modelo de texto para v√≠deo chamado KLING, e as pessoas est√£o enlouquecendo por isso.  Aqui est√£o 10 exemplos imperd√≠veis:  1. Um homem chin√™s senta √† mesa e come macarr√£o com hashis\n",
            "Digite um tweet para adicionar a lista de verifica√ß√£o: ‚Ä¢ Hoje √© dia de 'O que ele te diria?' lido com o teu signo solar\n",
            "Digite um tweet para adicionar a lista de verifica√ß√£o: Faz um carinho nesse baralho Van ü•≤\n",
            "Digite um tweet para adicionar a lista de verifica√ß√£o: De Camavinga para Mbapp√©... üî•   Vem logo temporada 2024/25.\n",
            "Digite um tweet para adicionar a lista de verifica√ß√£o: e a Vaidebet acaba de chegar em 900 mil seguidores de forma REL√ÇMPAGO.   depois da rescis√£o com o SCCP eles j√° tinham ficado abaixo de 700 mil seguidores e em apenas alguns minutos ganhaeam mais de 200 mil novos seguidores.   enquanto eu digitava eles ganharam mais MIL seguidores\n",
            "Digite um tweet para adicionar a lista de verifica√ß√£o: literalmente um dos piores dias que j√° vivenciei  eu acreditei, voc√™ acreditou, n√≥s t√≠nhamos esperan√ßa... momentos depois meu amor pelo futebol quase acaba por completo\n",
            "Digite um tweet para adicionar a lista de verifica√ß√£o: üá´üá∑ URGENTE: Ap√≥s derrota para a extrema-direita nas elei√ß√µes do Parlamento Europeu, Emmanuel Macron dissolve o Parlamento da Fran√ßa e convoca elei√ß√µes legislativas para o dia 30 de junho.\n",
            "Digite um tweet para adicionar a lista de verifica√ß√£o: E o Vinicius jr, melhor do mundo?  Atrai dois, sai na perna boa e cruza na cabe√ßa do Endrick.  Diferente.\n",
            "Digite um tweet para adicionar a lista de verifica√ß√£o: vai se fuder lixo do krl, joga nada\n",
            "Digite um tweet para adicionar a lista de verifica√ß√£o: esse a√≠ √© perna de pal, ruim pra krl, qualquer um joga mais que ele\n",
            "Digite um tweet para adicionar a lista de verifica√ß√£o: o timinho escroto esse a√≠\n",
            "Digite um tweet para adicionar a lista de verifica√ß√£o: Vai se fuder mano, ningu√©m pediu sua opini√£o lixo\n",
            "Digite um tweet para adicionar a lista de verifica√ß√£o: s\n",
            "O tweet: \"A China acabou de dar um grande passo √† frente em IA.  Eles lan√ßaram um modelo de texto para v√≠deo chamado KLING, e as pessoas est√£o enlouquecendo por isso.  Aqui est√£o 10 exemplos imperd√≠veis:  1. Um homem chin√™s senta √† mesa e come macarr√£o com hashis\", √© classificado como: \"n√£o t√≥xico\"\n",
            "O tweet: \"‚Ä¢ Hoje √© dia de 'O que ele te diria?' lido com o teu signo solar\", √© classificado como: \"n√£o t√≥xico\"\n",
            "O tweet: \"Faz um carinho nesse baralho Van ü•≤\", √© classificado como: \"n√£o t√≥xico\"\n",
            "O tweet: \"De Camavinga para Mbapp√©... üî•   Vem logo temporada 2024/25.\", √© classificado como: \"n√£o t√≥xico\"\n",
            "O tweet: \"e a Vaidebet acaba de chegar em 900 mil seguidores de forma REL√ÇMPAGO.   depois da rescis√£o com o SCCP eles j√° tinham ficado abaixo de 700 mil seguidores e em apenas alguns minutos ganhaeam mais de 200 mil novos seguidores.   enquanto eu digitava eles ganharam mais MIL seguidores\", √© classificado como: \"n√£o t√≥xico\"\n",
            "O tweet: \"literalmente um dos piores dias que j√° vivenciei  eu acreditei, voc√™ acreditou, n√≥s t√≠nhamos esperan√ßa... momentos depois meu amor pelo futebol quase acaba por completo\", √© classificado como: \"n√£o t√≥xico\"\n",
            "O tweet: \"üá´üá∑ URGENTE: Ap√≥s derrota para a extrema-direita nas elei√ß√µes do Parlamento Europeu, Emmanuel Macron dissolve o Parlamento da Fran√ßa e convoca elei√ß√µes legislativas para o dia 30 de junho.\", √© classificado como: \"n√£o t√≥xico\"\n",
            "O tweet: \"E o Vinicius jr, melhor do mundo?  Atrai dois, sai na perna boa e cruza na cabe√ßa do Endrick.  Diferente.\", √© classificado como: \"n√£o t√≥xico\"\n",
            "O tweet: \"vai se fuder lixo do krl, joga nada\", √© classificado como: \"t√≥xico\"\n",
            "O tweet: \"esse a√≠ √© perna de pal, ruim pra krl, qualquer um joga mais que ele\", √© classificado como: \"n√£o t√≥xico\"\n",
            "O tweet: \"o timinho escroto esse a√≠\", √© classificado como: \"n√£o t√≥xico\"\n",
            "O tweet: \"Vai se fuder mano, ningu√©m pediu sua opini√£o lixo\", √© classificado como: \"t√≥xico\"\n"
          ]
        }
      ]
    }
  ]
}